---
layout: page
title: Workshop
permalink: /workshop/
---

_Workshop on Theoretical Advances in Deep Learning_ will take place at the Istanbul Center for Mathematical Sciences. The venue is located inside the Bogazici University main campus.   

**Dates**  
Tuesday,Â July 30 - Friday, August 2, 2019   

**Location**  
Istanbul Center for Mathematical Sciences

<details>
    <summary>
        <b>
            About
        </b>
    </summary>
    <p markdown="1">  

        __Abstract:__ During the past few years, differentiable programming as a paradigm of deep learning provided cutting edge applications of machine learning in large scale problems in wide areas covering vision, speech, translation, and various autonomous machines. However, the success rate of working models is much faster than the scientific progress on understanding the working principles of such systems. More recently, theoretical developments shed some light on the inner workings of toy models on simple tasks, yet the community is still missing theoretical results that have strong predictive power on what to expect from large scale models on complex tasks and how to design them to improve their performance. In an attempt to move towards deeper understanding, we aim to bring together a group of researchers interested in the theoretical understanding of deep learning. The workshop is devoted to reviewing the most recent literature to bring everyone at the same level in terms of our current understanding, further, we will discuss theoretical challenges and propose ways to move forward. We will also devote one day of the workshop to interact with the local machine learning community that will include an opportunity for interested advanced students to introduce themselves and we will have a public lecture covering current trends in machine learning.  

    </p>
    <p>
        <p><strong>Topics:</strong></p>
        <ul>
        <li>Toy models that exhibit characteristic features of large scale systems  </li>
        <li>Scaling laws of neural networks with their degrees of freedom  </li>
        <li>Algorithmic effects and regularization in training neural networks  </li>
        <li>The role of the structure in data and teacher-student networks  </li>
        <li>Limiting behavior of simple models  </li>
        <li>Statistical physics approach to neural networks implications and its limits  </li> 
        <li>The role of priors on the performance of models </li>
        </ul>
    </p>
</details>

<details>
    <summary>
        <b markdown="1">
            Commute
        </b>
    </summary>
    <p markdown="1">  

        The subway station next to the campus is [Bogazici Universitesi Istasyonu](https://goo.gl/maps/VrC42pG9vi7u2vpW9). It is the last stop on line M6. Take line M2 (goes through Taksim) and transfer at the station called **Levent** (you can't miss it!).

        Commute to Bogazici University South Campus  
        ![commute](/assets/images/map1-.jpg)
        Closer look at the South Campus  
        ![campus](/assets/images/map2-.jpg)
        Zooming in on IMBM  
        ![venue](/assets/images/map3-.jpg)
       
    </p>
</details>


<details>
    <summary>
        <b markdown="1">
            Participants
        </b>
    </summary>
    <p>  
    <ul>
        <li>Ethem Alpaydin, Ozyegin University  </li>

        <li>Anima Anandkumar, Caltech &amp; NVIDIA  </li>

        <li>Benjamin Aubin, ENS  </li>

        <li>Aristide Baratin, MILA  </li>

        <li>David Belius, University of Basel  </li>

        <li>&dagger;Giulio Biroli, ENS  </li>

        <li>Chiara Cammarota, King's College London  </li>

        <li>Stephane d'Ascoli, ENS  </li>

        <li>Ethan Dyer, Google  </li>

        <li>Alp Eden, Bogazici University (retired)  </li>

        <li>Utku Evci, Google  </li>
        
        <li>Chiara Facciola, MOX - Politecnico di Milano  </li>

        <li>Orhan Firat, Google   </li>

        <li>Silvio Franz, Universite Paris-Sud  </li>

        <li>Marylou Gabrie, ENS  </li>

        <li>Surya Ganguli, Stanford   </li>
        
        <li>*Mario Geiger, EPFL  </li>

        <li>Caglar Gulcehre, DeepMind  (over VC) </li>

        <li>Mert Gurbuzbalaban, Rutgers Business School  </li>

        <li>Clement Hongler, EPFL  </li>

        <li>Sungmin Hwang, LPTMS  </li>

        <li>Melih Iseri, USC </li>  

        <li>Duygu Islakoglu, Koc University  </li>

        <li>Arthur Jacot, EPFL  </li>

        <li>Mehmet Kiral, Sophia University  </li>

        <li>Florent Krzakala, ENS  </li>

        <li>Fabian Latorre, EPFL  </li>

        <li>Ioannis Mitliagkas, MILA  </li>

        <li>Muhittin Mungan, Uni Bonn  </li>

        <li>Brady Neal, MILA  </li>

        <li>Behnam Neyshabur, NYU  </li>

        <li>Mihai Nica, University of Toronto  </li>

        <li>Ekin Ozman, Bogazici University  </li>

        <li>Vardan Papyan, Stanford  </li>

        <li>Dan Roberts, Diffeo Labs  </li>

        <li>Miguel Ruiz Garcia, University of Pennsylvania  </li>

        <li>&dagger;Levent Sagun, EPFL   </li>

        <li>Stefano Sarao, CEA   </li>

        <li>Andrew Saxe, University of Oxford  </li>

        <li>Berrenur Saylam, Bogazici University  </li>

        <li>David Schwab, CUNY  </li>

        <li>Berfin Simsek, EPFL  </li>

        <li>Sam Smith, DeepMind  </li>

        <li>Stefano Spigler, EPFL  </li>

        <li>Eric Vanden-Eijnden, NYU  </li>

        <li>&dagger;Matthieu Wyart, EPFL  </li>

        <li>Sho Yaida, Facebook AI  </li>

        <li>Lenka Zdeborova, CEA <br/>
        &dagger;: Organizers, *: To be confirmed</li>
    </ul>
    </p>
</details>



<details>
    <summary>
        <b markdown="1">
            Program
        </b>
    </summary>
    <p>  
        <table>
            <thead>
                <tr>
                    <th>Time</th>
                    <th style="text-align:left">Event</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>9:00-10:30</td>
                    <td style="text-align:left">Session 1</td>
                </tr>
                <tr>
                    <td>10:30-11:00</td>
                    <td style="text-align:left">Coffee Break</td>
                </tr>
                <tr>
                    <td>11:00-12:00</td>
                    <td style="text-align:left">Session 2</td>
                </tr>
                <tr>
                    <td>12:00-13:30</td>
                    <td style="text-align:left">Lunch Break</td>
                </tr>
                <tr>
                    <td>13:30-15:00</td>
                    <td style="text-align:left">Session 3</td>
                </tr>
                <tr>
                    <td>15:00-15:30</td>
                    <td style="text-align:left">Coffee Break</td>
                </tr>
                <tr>
                    <td>15:30-16:30</td>
                    <td style="text-align:left">Session 4</td>
                </tr>
            </tbody>
        </table>
        <p>Each session will include back-to-back 30-minute talks (25 + 5 for questions). During after hours and weekends, the venue will be available for informal discussions. The detailed schedule will be available soon.  </p>
    </p>
</details>  


<table>
    <thead>
        <tr>
            <th>Time</th>
            <th style="text-align:left">Tuesday, July 30</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>09:00-09:30</td>
            <td style="text-align:left">Orhan Firat, TBA</td>
        </tr>
        <tr>
            <td>09:30-10:00</td>
            <td style="text-align:left">Surya Ganguli, Understanding the first steps of vision through interpretable deep learning</td>
        </tr>
        <tr>
            <td>10:00-10:30</td>
            <td style="text-align:left">Coffee break</td>
        </tr>
        <tr>
            <td>10:30-11:00</td>
            <td style="text-align:left">Brady Neal, Over-Parametrization in Deep RL and Causal Graphs for Deep Learning Theory</td>
        </tr>
        <tr>
            <td>11:00-11:30</td>
            <td style="text-align:left">Florent Krzakala, TBA</td>
        </tr>
        <tr>
            <td>11:30-12:00</td>
            <td style="text-align:left">Joint Q&A and Discussion</td>
        </tr>
        <tr>
            <td>12:00-13:30</td>
            <td style="text-align:left">Lunch Break</td>
        </tr>
        <tr>
            <td>13:30-14:00</td>
            <td style="text-align:left">Sam Smith, Practical Insights into SGD Hyper-parameters</td>
        </tr>
        <tr>
            <td>14:00-14:30</td>
            <td style="text-align:left">Mert Gurbuzbalaban, A Tail-Index Analysis of Stochastic Gradient Noise in Deep Neural Networks</td>
        </tr>
        <tr>
            <td>14:30-15:00</td>
            <td style="text-align:left">Mihai Nica, Gradients of ReLU Networks on Initialization</td>
        </tr>
        <tr>
            <td>15:00-15:30</td>
            <td style="text-align:left">Coffee Break</td>
        </tr>
        <tr>
            <td>15:30-16:00</td>
            <td style="text-align:left">Berfin Simsek, Weight-space symmetry in deep networks gives rise to permutation saddles, connected by equal-loss valleys across the loss landscape</td>
        </tr>
        <tr>
            <td>16:00-16:30</td>
            <td style="text-align:left">Stefano Spigler, Asymptotic Learning Curves of Kernel Methods</td>
        </tr>
        <tr>
            <td>16:30-17:00</td>
            <td style="text-align:left">Joint Q&A and Discussion</td>
        </tr>
    </tbody>
</table>


<table>
    <thead>
        <tr>
            <th>Time</th>
            <th style="text-align:left">Wednesday, July 31</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>09:00-09:30</td>
            <td style="text-align:left">Clement Hongler, Neural Tangent Kernel</td>
        </tr>
        <tr>
            <td>09:30-10:00</td>
            <td style="text-align:left">Eric Vanden-Eijnden, Trainability and Accuracy of Neural Networks: An Interacting Particle System Approach</td>
        </tr>
        <tr>
            <td>10:00-10:30</td>
            <td style="text-align:left">Coffee break</td>
        </tr>
        <tr>
            <td>10:30-11:00</td>
            <td style="text-align:left">Mario Geiger (presented by Matthieu Wyart), On the benefits of over-parametrisation: Lazy vs feature learning </td>
        </tr>
        <tr>
            <td>11:00-11:30</td>
            <td style="text-align:left">Arthur Jacot, Freeze and Chaos for DNNs: an NTK view of Batch Normalization, Checkerboard and Boundary Effects</td>
        </tr>
        <tr>
            <td>11:30-12:00</td>
            <td style="text-align:left">Joint Q&A and Discussion</td>
        </tr>
        <tr>
            <td>12:00-13:30</td>
            <td style="text-align:left">Lunch Break</td>
        </tr>
        <tr>
            <td>13:30-14:00</td>
            <td style="text-align:left">Andrew Saxe, High-dimensional dynamics of generalization error in neural networks</td>
        </tr>
        <tr>
            <td>14:00-14:30</td>
            <td style="text-align:left">Ioannis Mitliagkas, A Modern Take on the Bias-Variance Tradeoff in Neural Networks</td>
        </tr>
        <tr>
            <td>14:30-15:00</td>
            <td style="text-align:left">Behnam Neyshabur, TBA</td>
        </tr>
        <tr>
            <td>15:00-15:30</td>
            <td style="text-align:left">Coffee Break</td>
        </tr>
        <tr>
            <td>15:30-16:00</td>
            <td style="text-align:left">Aristide Baratin, Rethinking Complexity in Deep Learning: A View from Function Space</td>
        </tr>
        <tr>
            <td>16:00-16:30</td>
            <td style="text-align:left">Stephane d'Ascoli, TBA</td>
        </tr>
        <tr>
            <td>16:30-17:00</td>
            <td style="text-align:left">Joint Q&A and Discussion</td>
        </tr>
    </tbody>
</table>
